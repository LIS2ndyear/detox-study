{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "860e28a6-4587-4a2e-b649-eab5ece2d5d9",
   "metadata": {},
   "source": [
    "# Emotional and Linguistic Framing of Digital Detox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f043430f-bd2b-4879-9723-93e11f8e2260",
   "metadata": {},
   "source": [
    "### Notebook 4: BERTopicEmotion and Sentiment Analysis of Text Using TF-IDF, VAD Lexicon, TextBlob, and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ed057f-e0df-43d6-98ab-c66c1f2835a7",
   "metadata": {},
   "source": [
    "This notebook performs emotion and sentiment analysis on Reddit posts using:\r\n",
    "\r\n",
    "- **TF-IDF + VAD Lexicon**: Captures emotional norms (Valence, Arousal, Dominance) at word level.\r\n",
    "- **TextBlob**: Provides polarity and subjectivity scores on raw text.\r\n",
    "- **PCA**: Reduces the high-dimensional emotion space into two principal components for visualization.\r\n",
    "\r\n",
    "The analysis helps understand how emotions are thematically expressed across posts labeled as digital detox vs. general/control.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f5aa845-ac71-4766-a5b9-488b641f6ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.5.2)\n",
      "Collecting textblob\n",
      "  Using cached textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: nltk>=3.9 in /opt/conda/lib/python3.11/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/conda/lib/python3.11/site-packages (from seaborn) (3.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk>=3.9->textblob) (4.66.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.19.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# required packages\n",
    "!pip install pandas scikit-learn textblob seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85211d09-4ed9-40c3-a044-b78fc24bedf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detox and control datasets\n",
    "detox_df = pd.read_csv(\"/home/jovyan/XXX/Back up/XXX/detox_with_topics.csv\")\n",
    "control_df = pd.read_csv(\"/home/jovyan/XXX/Back up/XXX/control_with_topics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80c3094-728e-425a-8273-39135463e858",
   "metadata": {},
   "source": [
    "### Calculate VAD from Tokens (unweighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b09b8b-802e-4829-b89e-4e89adf568cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from notebook - data classification\n",
    "# load NRC VAD Lexicon\n",
    "vad_nrc = pd.read_csv(\"/home/jovyan/XXX/Back up/NRC-VAD-Lexicon-v2.1.txt\", sep='\\t')\n",
    "vad_nrc.columns = vad_nrc.columns.str.lower()\n",
    "vad_nrc.set_index('term', inplace=True)\n",
    "vad_nrc.index = vad_nrc.index.str.lower()\n",
    "vad_nrc = vad_nrc.dropna(subset=['valence', 'arousal', 'dominance'])\n",
    "\n",
    "# store for lookup\n",
    "vad_norms_data = vad_nrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48b182df-cd6d-4e29-bd2e-8ec4e9b0d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure VAD lexicon index is lowercase\n",
    "vad_norms_data.index = vad_norms_data.index.str.lower()\n",
    "\n",
    "def avg_vad_for_tokens(tokens):\n",
    "    tokens = [t.lower() for t in tokens if isinstance(t, str)]\n",
    "    vad_values = vad_norms_data.loc[vad_norms_data.index.intersection(tokens)]\n",
    "    if not vad_values.empty:\n",
    "        return vad_values.mean()\n",
    "    else:\n",
    "        return pd.Series({'valence': None, 'arousal': None, 'dominance': None})\n",
    "\n",
    "# apply to df\n",
    "for df in [detox_df, control_df]:\n",
    "    df[['valence', 'arousal', 'dominance']] = df['body_tokens'].apply(avg_vad_for_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df80fda-3f6d-4a99-8feb-9544ae681ba8",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cec9f4d-d13f-4c12-8612-6b24710f82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a string representation of a list back into list\n",
    "def str_to_list(s):\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def fix_token_string(token_string):\n",
    "    tokens = str_to_list(token_string)\n",
    "    return ' '.join([''.join(token.split()) for token in tokens])\n",
    "\n",
    "# apply to both df\n",
    "for df in [detox_df, control_df]:\n",
    "    df['body_tokens'] = df['body_tokens'].apply(str_to_list)\n",
    "    df['clean_text'] = df['body_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# combine all posts and labels\n",
    "combined_df = pd.concat([control_df, detox_df], ignore_index=True)\n",
    "all_texts = combined_df['clean_text'].tolist()\n",
    "labels = ['control'] * len(control_df) + ['detox'] * len(detox_df)\n",
    "combined_df['label'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb27933-e0fd-46eb-8fc1-3d42b6a1b723",
   "metadata": {},
   "source": [
    "### Combine TF-IDF Ã— VAD (VAD-weighted TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9db8d94-dd25-4c63-9509-f1d88ffa8f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9, min_df=2)\n",
    "tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vocab)\n",
    "\n",
    "# filter to words present in VAD\n",
    "common_words = df_tfidf.columns.intersection(vad_nrc.index)\n",
    "tfidf_common = df_tfidf[common_words]\n",
    "vad_common = vad_nrc.loc[common_words]\n",
    "\n",
    "# multiply TF-IDF with VAD\n",
    "tfidf_valence = tfidf_common * vad_common['valence'].values\n",
    "tfidf_arousal = tfidf_common * vad_common['arousal'].values\n",
    "tfidf_dominance = tfidf_common * vad_common['dominance'].values\n",
    "\n",
    "# aggregate document-level emotion scores\n",
    "df_vad_weighted = pd.DataFrame({\n",
    "    'valence': tfidf_valence.sum(axis=1),\n",
    "    'arousal': tfidf_arousal.sum(axis=1),\n",
    "    'dominance': tfidf_dominance.sum(axis=1),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c0a029-bd4f-4c58-879e-80f6ce826a04",
   "metadata": {},
   "source": [
    "### TextBlob sentiment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec1cb90-a7a9-4c7e-9b0d-c45a1926c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply textblob in for each row of a dataframe. (n.d.). Stack Overflow. https://stackoverflow.com/questions/43485469/apply-textblob-in-for-each-row-of-a-dataframe\n",
    "# calculate sentiment polarity and subjectivity for each cleaned text using textblob\n",
    "combined_df['polarity'] = combined_df['clean_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "combined_df['subjectivity'] = combined_df['clean_text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "df_vad_weighted['polarity'] = combined_df['polarity'].values\n",
    "df_vad_weighted['subjectivity'] = combined_df['subjectivity'].values\n",
    "df_vad_weighted['label'] = combined_df['label'].values\n",
    "df_vad_weighted['topic'] = combined_df['topic'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0485d65-e861-4045-a7b3-2a26a19440c2",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c41185f9-0cd0-482e-81f2-aab4d4aadb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from NLP revison notebook\n",
    "# drop any rows with missing values\n",
    "df_vad_clean = df_vad_weighted.dropna(subset=['valence', 'arousal', 'dominance'])\n",
    "\n",
    "# run PCA\n",
    "pca = PCA(n_components=2)\n",
    "vad_features = df_vad_clean[['valence', 'arousal', 'dominance']]\n",
    "vad_pca = pca.fit_transform(vad_features)\n",
    "\n",
    "# create PCA df\n",
    "df_pca = pd.DataFrame(vad_pca, columns=['PC1', 'PC2'])\n",
    "df_pca['label'] = df_vad_clean['label'].values\n",
    "df_pca['topic'] = df_vad_clean['topic'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "587ec286-d28c-46f5-a61e-18f90f236c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vad_weighted.to_csv(\"df_vad_weighted_with_sentiment.csv\", index=False)\n",
    "df_pca.to_csv(\"df_vad_pca.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
